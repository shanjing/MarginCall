# This is global configuraton file for the repo
# When using Cloud LLM, just complete 1 & 2 which will override local llm settings
# If also use local llm via ollama,
# just comment out the CLOUD_AI_MODEL and set LOCAL_AI_MODEL variable


# 1. A valid GOOGLE_API_KEY is required to run gemini models
# Go to https://aistudio.google.com/api-keys to create an API Key and paste it below
GOOGLE_API_KEY='Google AI Studio API Key'
GOOGLE_GENAI_USE_VERTEXAI=false

# 2. If this variable is setup the agent will use a cloud based model
# other models germini-2.5-flash gemini-2.5-pro
CLOUD_AI_MODEL='gemini-2.5-flash'
# Use Google AI Studio API key (GOOGLE_API_KEY) with no GCP/Vertex (see README.md)
GOOGLE_GENAI_USE_VERTEXAI=false


# 3. To use local model, comment out CLOUD_AI_MODEL
LOCAL_AI_MODEL='ollama_chat/qwen3-coder-next'
LOCAL_AI_MODEL_NOTOOLS='ollama_chat/gemma3:27b'
LOCAL_AI_MODEL_1='ollama_chat/gpt-oss:20b'
LOCAL_AI_MODEL_CODING='ollama_chat/qwen3:32b'
LOCAL_AI_MODEL_REASONING='ollama_chat/aia/DeepSeek-R1-Distill-Qwen-32B-Uncensored-i1:latest'

# 4. Set ollama routing for local llm, change below with your correct ollama setting
#OLLAMA_API_BASE='http://localhost:11434'

# 5. APP settings
AGENT_APP_NAME='MarginCall'
USER_ID='Trader'
AGENT_ENV='development'
ROOT_AGENT='stock_analyst'
SUB_AGENTS='stock_analysis_pipeline stock_data_collector report_synthesizer presenter news_fetcher'

#7. Docker container settings
PORT=8080

#8. Third party API keys
#Optionally get an API key from brave.com if use search function in local llms
BRAVE_API_KEY='brave.com API key'
#Optionallly get an AgentOps API Key for observabilities
AGENTOPS_API_KEY='agentops.ai'
