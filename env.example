# This is global configuraton file for the repo
# When using Cloud LLM, just complete 1 & 2 which will override local llm settings
# If also use local llm via ollama, 
# just comment out the CLOUD_AI_MODEL and set LOCAL_AI_MODEL variable


# 1. A valid GOOGLE_API_KEY is required to run gemini models
# Go to https://aistudio.google.com/api-keys to create an API Key and paste it below
GOOGLE_API_KEY="Google AIStudio API Key"
# Required when using API key (e.g. in Docker): use Google AI Studio, not Vertex AI
GOOGLE_GENAI_USE_VERTEXAI=false

# 2. If this variable is setup the agent will use a cloud based model
# other models germini-2.5-flash gemini-2.5-pro
CLOUD_AI_MODEL="gemini-2.5-flash"


# 3. To use local model, comment out CLOUD_AI_MODEL
LOCAL_AI_MODEL="ollama_chat/qwen3-coder-next"
LOCAL_AI_MODEL_NOTOOLS="ollama_chat/gemma3:27b"
LOCAL_AI_MODEL_1="ollama_chat/gpt-oss:20b"
LOCAL_AI_MODEL_CODING="ollama_chat/qwen3:32b"
LOCAL_AI_MODEL_REASONING="ollama_chat/aia/DeepSeek-R1-Distill-Qwen-32B-Uncensored-i1:latest"

# 4. Set ollama routing for local llm, change below with your correct ollama setting
#OLLAMA_API_BASE="http://localhost:11434"

# 5. Optionally get an API key from brave.com if use search function in local llms
BRAVE_API_KEY="brave.com API key"

# 6. APP settings
AGENT_APP_NAME="MarginCall"
USER_ID="Trader"
AGENT_ENV="development"
ROOT_AGENT="stock_analyst"
SUB_AGENTS="stock_analysis_pipeline stock_data_collector report_synthesizer presenter news_fetcher"

#7. Docker container settings
PORT=8080
